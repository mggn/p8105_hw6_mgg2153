---
title: "p8105_hw6_mgg2153"
author: "mggn"
date: "12/8/2020"
output: github_document
---
### Final data science homework of the semester!

Thank you, teaching team, for a great class. 

Ok, let's get started:

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(p8105.datasets)
```
Problem 1 was completed in class, so I'll jump right in to problem 2:

### Problem 2

Using the birthweight data set, we will (1) import the data (2) tidy the data and (3) check for missing using a neat little function I found by googling

```{r load_data}

bwt_df = 
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    mrace = as.factor(mrace),
    frace = as.factor(frace),
    babysex = as.factor(babysex),
    malform = as.factor(malform)
  )

sapply(bwt_df, function(x) sum(is.na(x))) #check missing
```
From the output, we can see that there are no missing values! How nice.

#### Building a model

We are looking to construct a regression model to estimate birthweight. Looking at the codebook, my variables of interest are:

- gestational age (weeks)  
- maternal race   
- pre-pregnancy BMI  
- SES (income used as proxy here)  
- maternal age at delivery

**why?**

As part of my thesis, I was looking for birth outcomes after maternal exposure to stress from natural disaster. The process for model building included a theoretical framework as presented by the following DAG, and influenced by other literature relating prenatal stress to birth outcomes, namely, birthweight

<img src="images/Sandy_DAG.png" style="width:85%">


Of course, the data we have here are different: we are looking at factors that influence birth outcomes NOT in the context of natural disaster!
Nevertheless, the DAG and the literature point to the variables I listed above as being influential in determining birthweight. Thus, I included them in my regression model below. Additionally, I generated a plot of the residuals generated from my model vs. the predicted values for the regression diagnostics plot

```{r my_model}

mg_model = lm(bwt ~ gaweeks+mrace+ppbmi+fincome+momage, data = bwt_df)

```

Model estimates are summarized below:

```{r}
mg_model %>% broom::tidy()
```

And here is the plot that I mentioned above!

```{r}
bwt_df %>%
  modelr::add_predictions(mg_model) %>%
  modelr::add_residuals(mg_model) %>%
  ggplot(aes(x = pred, y = resid))+geom_point()+#look back at regression notes :D
  geom_smooth(se = FALSE, method = "lm", color = "green")+
  theme_bw()+
  labs(
    x = "Residuals",
    y = "Predicted values",
    title = "Residuals vs. predicted values", #yhat?
    caption = "Regression diagnostics"
  )


```
Looking at the plot, it looks like the residuals are symmetrical around the line at y = 0. This kind of plot can be used to assess whether the residuals from your regression are normally distributed. Since the points are symmetrical, aka evenly distributed, it seems like the residuals are evenly distributed and thus the normal distribution of residuals assumption is satisfied for linear regression.

#### Model comparisons, cross-validation using modelr

Now we will cross-validate three models; the two below with the model I constructed above

(1)
```{r regression}

#main effects model

main_effect = lm(bwt ~ blength + gaweeks, data = bwt_df)

main_effect %>% broom::tidy()


```

(2)
```{r}

#head circumference, length, sex, and all interactions
saturated = lm(bwt ~ bhead*blength*babysex, data = bwt_df)

saturated %>% broom::tidy()

```

**cross validation step**

```{r}

cv_df =
  crossv_mc(bwt_df, 100)

cv_df = 
cv_df %>%
  mutate(
    mg_model = map(train, ~lm(bwt ~ gaweeks+mrace+ppbmi+fincome+momage, data = .x)),
    main_effect = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    saturated = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))
  ) %>%
  mutate(
    rmse_mg_model = map2_dbl(mg_model, test, ~rmse(model = .x, data = .y)),
    rmse_main_effect = map2_dbl(main_effect, test, ~rmse(model = .x, data = .y)),
    rmse_saturated = map2_dbl(saturated, test, ~rmse(model = .x, data = .y))
  )

```

Look at distribution of RMSEs to see which model is best

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot(aes(fill=model))+
  theme_bw()+
  labs(
    x = "Model",
    y = "RMSE",
    title = "RMSE distribution across 3 models"
  )+
  theme(legend.position = "none")

```

Looks like the clear winner is the saturated model(we want the smallest RMSE), with the three-way interaction!

### Problem 3

Let's load in the noaa data

```{r load_noaa}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

set.seed(1)

```

Lets use bootstrap and generate models for tmax = tmin, then let's pull the estimates of the intercepts (b0) and the slopes (b1) and create the log of the product of these two.
Note: R uses log to mean natural log, and log10 for log base 10. Since this is a biostats class, let's use log aka natural log :-)

Below is the process to summarize the 95% confidence interval

```{r}
weather_df %>%
  bootstrap(n = 5000)%>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
  ) %>%
  select(-strap, -models) %>%
  unnest(results) %>%
  select(.id, term, estimate)%>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  )%>%
  janitor::clean_names()%>%
  mutate(
    log_b_hats = log(intercept*tmin)
  )%>%
  summarize(
    lower_limit = quantile(log_b_hats, c(.025)),
    upper_limit = quantile(log_b_hats, c(.975))
  )%>%
  knitr::kable()
```


And now we follow a similar process for the r hat squared

```{r}
weather_df %>%
  bootstrap(n = 5000)%>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)
  ) %>%
  select(-strap, -models) %>%
  unnest(results) %>%
  janitor::clean_names()%>%
  summarize(
    lower_limit = quantile(r_squared, c(.025)),
    upper_limit = quantile(r_squared, c(.975))
  )%>% knitr::kable()

```

*and last but not least...* PLOTS of the distributions of the estimates!

```{r}
weather_df %>%
  bootstrap(n = 5000)%>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
  ) %>%
  select(-strap, -models) %>%
  unnest(results) %>%
  select(.id, term, estimate)%>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  )%>%
  janitor::clean_names()%>%
  mutate(
    log_b_hats = log(intercept*tmin)
  )%>%
  ggplot(aes(x = log_b_hats))+geom_density()+theme_bw()+
  labs(
    x = "log(b0*b1)",
    y = "Density",
    title = "Distribution of [simulated] log(b0*b1) values"
  )

```

```{r}
weather_df %>%
  bootstrap(n = 5000)%>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)
  ) %>%
  select(-strap, -models) %>%
  unnest(results) %>%
  janitor::clean_names()%>%
  ggplot(aes(x = r_squared))+geom_density()+theme_bw()+
  labs(
    x = "R (hat) squared",
    y = "Density",
    title = "Distribution of [simulated] R (hat) squared values"
  )


```

Both plots look like a good old normal distribution :o)
